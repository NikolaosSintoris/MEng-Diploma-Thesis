{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward NN with:\n",
    "# 1 hidden layer with self.first_hidden_size neurons and Relu activation function.\n",
    "# 1 output layer with 3 neurons and softmax activation function.\n",
    "class MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size, first_hidden_size):\n",
    "            super(MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.first_hidden_size  = first_hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.first_hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.first_hidden_size, 3)\n",
    "            self.softmax = torch.nn.Softmax(dim = 1)\n",
    "            \n",
    "            \n",
    "        def forward(self, x):\n",
    "            first_hidden = self.fc1(x)\n",
    "            first_relu = self.relu(first_hidden)\n",
    "            \n",
    "            output = self.fc2(first_relu)\n",
    "            output = self.softmax(output)\n",
    "            return output   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the accuracy.\n",
    "def compute_accuracy(predictions, labels):\n",
    "    # Take the max value and the max index of every vector in predictions.\n",
    "    max_values, max_indexes = torch.max(predictions.data, 1)\n",
    "    # Compute the accuracy.\n",
    "    accuracy = (((max_indexes == labels).sum().item()) / labels.shape[0]) * 100\n",
    "    return accuracy\n",
    "\n",
    "def test_model(model, data, actual_labels):\n",
    "    # Set evaluation mode.\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute predicted labels.\n",
    "    predicted_test_labels = model(data)\n",
    "    \n",
    "    # Compute accuracy.\n",
    "    final_accuracy = compute_accuracy(predicted_test_labels, actual_labels)\n",
    "    \n",
    "    return final_accuracy, predicted_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Case Name:  \\case01\n",
      "\tTest data shape:  torch.Size([813, 2048])\n",
      "\tActual test labels shape:  torch.Size([813])\n",
      "\tTest accuracy:  91.26691266912668\n",
      "\tTest F1-Score:  0.7861918891219609\n",
      "Current Case Name:  \\case02\n",
      "\tTest data shape:  torch.Size([609, 2048])\n",
      "\tActual test labels shape:  torch.Size([609])\n",
      "\tTest accuracy:  61.412151067323485\n",
      "\tTest F1-Score:  0.5891802615189787\n",
      "Current Case Name:  \\case03\n",
      "\tTest data shape:  torch.Size([1305, 2048])\n",
      "\tActual test labels shape:  torch.Size([1305])\n",
      "\tTest accuracy:  89.65517241379311\n",
      "\tTest F1-Score:  0.6501738726074174\n",
      "Current Case Name:  \\case04\n",
      "\tTest data shape:  torch.Size([609, 2048])\n",
      "\tActual test labels shape:  torch.Size([609])\n",
      "\tTest accuracy:  98.0295566502463\n",
      "\tTest F1-Score:  0.8832184197039\n",
      "Current Case Name:  \\case05\n",
      "\tTest data shape:  torch.Size([753, 2048])\n",
      "\tActual test labels shape:  torch.Size([753])\n",
      "\tTest accuracy:  89.9070385126162\n",
      "\tTest F1-Score:  0.8601644607546021\n",
      "Current Case Name:  \\case06\n",
      "\tTest data shape:  torch.Size([498, 2048])\n",
      "\tActual test labels shape:  torch.Size([498])\n",
      "\tTest accuracy:  80.72289156626506\n",
      "\tTest F1-Score:  0.6630355250015998\n",
      "Current Case Name:  \\case07\n",
      "\tTest data shape:  torch.Size([388, 2048])\n",
      "\tActual test labels shape:  torch.Size([388])\n",
      "\tTest accuracy:  94.3298969072165\n",
      "\tTest F1-Score:  0.7555922051526793\n",
      "Current Case Name:  \\case08\n",
      "\tTest data shape:  torch.Size([176, 2048])\n",
      "\tActual test labels shape:  torch.Size([176])\n",
      "\tTest accuracy:  94.88636363636364\n",
      "\tTest F1-Score:  0.9017172419789584\n",
      "Current Case Name:  \\case09\n",
      "\tTest data shape:  torch.Size([1017, 2048])\n",
      "\tActual test labels shape:  torch.Size([1017])\n",
      "\tTest accuracy:  90.56047197640117\n",
      "\tTest F1-Score:  0.7417921783650826\n",
      "Current Case Name:  \\case10\n",
      "\tTest data shape:  torch.Size([572, 2048])\n",
      "\tActual test labels shape:  torch.Size([572])\n",
      "\tTest accuracy:  89.86013986013987\n",
      "\tTest F1-Score:  0.7262761318594801\n",
      "Current Case Name:  \\case11\n",
      "\tTest data shape:  torch.Size([124, 2048])\n",
      "\tActual test labels shape:  torch.Size([124])\n",
      "\tTest accuracy:  77.41935483870968\n",
      "\tTest F1-Score:  0.6818755015476327\n",
      "Current Case Name:  \\case12\n",
      "\tTest data shape:  torch.Size([102, 2048])\n",
      "\tActual test labels shape:  torch.Size([102])\n",
      "\tTest accuracy:  87.25490196078431\n",
      "\tTest F1-Score:  0.7914267699430692\n",
      "Current Case Name:  \\case13\n",
      "\tTest data shape:  torch.Size([558, 2048])\n",
      "\tActual test labels shape:  torch.Size([558])\n",
      "\tTest accuracy:  85.12544802867383\n",
      "\tTest F1-Score:  0.7337073930599111\n",
      "Current Case Name:  \\case14\n",
      "\tTest data shape:  torch.Size([410, 2048])\n",
      "\tActual test labels shape:  torch.Size([410])\n",
      "\tTest accuracy:  66.09756097560975\n",
      "\tTest F1-Score:  0.5520333698634793\n",
      "Current Case Name:  \\case15\n",
      "\tTest data shape:  torch.Size([115, 2048])\n",
      "\tActual test labels shape:  torch.Size([115])\n",
      "\tTest accuracy:  93.04347826086956\n",
      "\tTest F1-Score:  0.9112932604735883\n",
      "Current Case Name:  \\case16\n",
      "\tTest data shape:  torch.Size([263, 2048])\n",
      "\tActual test labels shape:  torch.Size([263])\n",
      "\tTest accuracy:  83.26996197718631\n",
      "\tTest F1-Score:  0.8133209383209383\n",
      "Current Case Name:  \\case17\n",
      "\tTest data shape:  torch.Size([1289, 2048])\n",
      "\tActual test labels shape:  torch.Size([1289])\n",
      "\tTest accuracy:  89.52676493405741\n",
      "\tTest F1-Score:  0.8308776645803229\n",
      "Current Case Name:  \\case18\n",
      "\tTest data shape:  torch.Size([175, 2048])\n",
      "\tActual test labels shape:  torch.Size([175])\n",
      "\tTest accuracy:  80.57142857142857\n",
      "\tTest F1-Score:  0.7375087263369768\n",
      "Current Case Name:  \\case19\n",
      "\tTest data shape:  torch.Size([437, 2048])\n",
      "\tActual test labels shape:  torch.Size([437])\n",
      "\tTest accuracy:  86.49885583524028\n",
      "\tTest F1-Score:  0.8311139251749736\n",
      "Current Case Name:  \\case20\n",
      "\tTest data shape:  torch.Size([574, 2048])\n",
      "\tActual test labels shape:  torch.Size([574])\n",
      "\tTest accuracy:  82.92682926829268\n",
      "\tTest F1-Score:  0.8180408745235122\n",
      "Current Case Name:  \\case21\n",
      "\tTest data shape:  torch.Size([477, 2048])\n",
      "\tActual test labels shape:  torch.Size([477])\n",
      "\tTest accuracy:  87.0020964360587\n",
      "\tTest F1-Score:  0.6891232535113477\n",
      "Current Case Name:  \\case22\n",
      "\tTest data shape:  torch.Size([606, 2048])\n",
      "\tActual test labels shape:  torch.Size([606])\n",
      "\tTest accuracy:  91.41914191419141\n",
      "\tTest F1-Score:  0.7789475246272682\n",
      "Current Case Name:  \\case23\n",
      "\tTest data shape:  torch.Size([706, 2048])\n",
      "\tActual test labels shape:  torch.Size([706])\n",
      "\tTest accuracy:  90.93484419263456\n",
      "\tTest F1-Score:  0.84216503761701\n"
     ]
    }
   ],
   "source": [
    "# A list that has the names of all cases.\n",
    "cases_folder_names = ['\\\\case01', '\\\\case02', '\\\\case03', '\\\\case04', '\\\\case05', '\\\\case06', '\\\\case07', '\\\\case08', \n",
    "                      '\\\\case09', '\\\\case10', '\\\\case11', '\\\\case12', '\\\\case13', '\\\\case14', '\\\\case15', '\\\\case16',\n",
    "                      '\\\\case17', '\\\\case18', '\\\\case19', '\\\\case20', '\\\\case21', '\\\\case22', '\\\\case23']\n",
    "\n",
    "embeddings_directory = r\"C:\\Nikolaos Sintoris\\Education\\MEng CSE - UOI\\Diploma Thesis\\Training Results\\Layer 4\\Embeddings\\ResNet50 Classifier\"\n",
    "best_model_directory = r\"C:\\Nikolaos Sintoris\\Education\\MEng CSE - UOI\\Diploma Thesis\\Training Results\\Layer 4\\Best Model\\ResNet50 Classifier\"\n",
    "for current_case_name in cases_folder_names:\n",
    "    print(\"Current Case Name: \", current_case_name)\n",
    "    \n",
    "    #===============================================================\n",
    "    #==================== Load Test Dataset ==========================\n",
    "    current_case_embeddings_directory = embeddings_directory + current_case_name\n",
    "    \n",
    "    test_data_np = np.loadtxt(current_case_embeddings_directory + '\\\\test_data.csv', delimiter = ',')\n",
    "    test_data = torch.from_numpy(test_data_np) # It does not create a copy. Uses the same memory.\n",
    "    test_data = test_data.float()\n",
    "    print(\"\\tTest data shape: \", test_data.shape)\n",
    "\n",
    "    actual_test_labels_np = np.loadtxt(current_case_embeddings_directory + '\\\\actual_test_labels.csv', delimiter = ',')\n",
    "    actual_test_labels = torch.from_numpy(actual_test_labels_np) # It does not create a copy. Uses the same memory.\n",
    "    actual_test_labels = actual_test_labels.long()\n",
    "    print(\"\\tActual test labels shape: \", actual_test_labels.shape)\n",
    "    #===============================================================\n",
    "    #===============================================================\n",
    "    \n",
    "    #===============================================================\n",
    "    #==================== Load Best Model ==========================\n",
    "    current_case_best_model_directory = best_model_directory + current_case_name + \"\\\\state_dict_model.pt\"\n",
    "    \n",
    "    # Load the model with the maximum validation accuracy.\n",
    "    input_layer_size = test_data.shape[1]\n",
    "    first_hidden_layer_size = round(math.sqrt(input_layer_size * 3))\n",
    "    my_model = MLP(input_layer_size, first_hidden_layer_size)\n",
    "    my_model.load_state_dict(torch.load(current_case_best_model_directory))\n",
    "    #===============================================================\n",
    "    #===============================================================\n",
    "    \n",
    "    #===============================================================\n",
    "    #==================== Compute Accuracy ==========================\n",
    "    test_accuracy, predicted_test_labels = test_model(my_model, test_data, actual_test_labels)\n",
    "    print(\"\\tTest accuracy: \", test_accuracy)\n",
    "    #===============================================================\n",
    "    #===============================================================\n",
    "    \n",
    "    #===============================================================\n",
    "    #==================== Compute F1 Score ==========================\n",
    "    _, final_predicted_test_labels = torch.max(predicted_test_labels.data, 1)\n",
    "    final_predicted_test_labels_np = final_predicted_test_labels.numpy()\n",
    "\n",
    "    test_f1_score = f1_score(actual_test_labels_np, final_predicted_test_labels_np, average = 'macro')\n",
    "    print(\"\\tTest F1-Score: \", test_f1_score)\n",
    "    #===============================================================\n",
    "    #===============================================================\n",
    "    \n",
    "    #===============================================================\n",
    "    #==================== Save Results ==========================\n",
    "    # Save test accuracy and F1-score to a csv file.\n",
    "    test_results_np = np.array( [test_accuracy, test_f1_score] )\n",
    "    np.savetxt(best_model_directory + current_case_name + \"\\\\accuracy_f1_score.csv\", test_results_np, delimiter = ',')\n",
    "    #===============================================================\n",
    "    #==============================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
